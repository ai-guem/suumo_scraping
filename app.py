from time import sleep
from bs4 import BeautifulSoup
import requests
import pandas as pd
import schedule
import time
import re
import os
from datetime import datetime

url = 'https://suumo.jp/jj/chintai/ichiran/FR301FC001/?ar=030&bs=040&pc=30&smk=&po1=25&po2=99&kz=1&kz=2&tc=0400104&shkr1=03&shkr2=03&shkr3=03&shkr4=03&rn=0220&ek=022039270&ek=022033150&ek=022005650&ek=022010720&ek=022038540&ek=022030290&ek=022040940&ra=014&cb=0.0&ct=7.5&co=1&md=02&md=03&md=04&ts=1&ts=2&et=15&mb=20&mt=9999999&cn=25&tc=0400301&tc=0400101&tc=0400501&tc=0400601&tc=0401106&fw2='

url_1 = url.format(1)
r_1 = requests.get(url_1)
soup_1 = BeautifulSoup(r_1.text, 'html.parser')

# ç‰©ä»¶æ•°ã®å–å¾—
house_num_text = soup_1.find('div', class_='paginate_set-hit').text
num = int(re.sub(r"\D", "", house_num_text))

# ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šã®ä»¶æ•°
num_per_page_select = soup_1.find("select", {"id": "js-tabmenu1-pcChange"})
selected_option = num_per_page_select.find("option", selected=True)
num_per_page = int(selected_option.get("value"))

d_list = []

# ãƒšãƒ¼ã‚¸æ•°ã®è¨ˆç®—
page = (num // num_per_page) + (1 if num % num_per_page != 0 else 0)
print(f"ãƒšãƒ¼ã‚¸æ•°: {page}")

# ãƒ˜ãƒƒãƒ€ãƒ¼å½è£…ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ã£ã½ãã™ã‚‹ï¼‰
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'
}

# ç¯‰å¹´æ•°ã‚’æ–‡å­—åˆ—ã‹ã‚‰æŠ½å‡ºã™ã‚‹é–¢æ•°
def extract_build_year(text):
    """
    ç¯‰å¹´æ•°ã‚’æ–‡å­—åˆ—ã‹ã‚‰æŠ½å‡ºã™ã‚‹é–¢æ•°
    ä¾‹: 'ç¯‰37å¹´3éšå»º' -> 37
    """
    match = re.search(r'ç¯‰(\d+)å¹´', text)
    if match:
        return str('ç¯‰') + str(match.group(1)) + str('å¹´')
    else:
        return None  # è©²å½“ã—ãªã„å ´åˆã¯ None ã‚’è¿”ã™

# éšæ•°ã‚’æ–‡å­—åˆ—ã‹ã‚‰æŠ½å‡ºã™ã‚‹é–¢æ•°
def extract_floor(text):
    match = re.search(r'(\d+)éš', text)
    if match:
        return str(match.group(1)) + str('éš')
    else:
        return None  # è©²å½“ã—ãªã„å ´åˆã¯ None ã‚’è¿”ã™

# ç‰©ä»¶ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
for i in range(1, page + 1):
    target_url = url.format(i)
    print(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­: {i} / {page}")
    sleep(2)
    res = requests.get(target_url, headers=headers)
    soup = BeautifulSoup(res.text, 'html.parser')

    contents = soup.find_all('div', class_='cassetteitem')

    # å„ç‰©ä»¶ã®æƒ…å ±ã‚’æŠ½å‡º
    for content in contents:
        # ç‰©ä»¶å
        title_tag = content.find('div', class_='cassetteitem_content-title')
        title = title_tag.text.strip() if title_tag else 'N/A'
        # é§…ãƒ»å¾’æ­©
        station_tag = content.find('li', class_='cassetteitem_detail-col2')
        station = station_tag.text.strip() if station_tag else 'N/A'
        # ç¯‰å¹´æ•°
        age_tag = content.find('li', class_='cassetteitem_detail-col3')
        age = age_tag.text.strip() if age_tag else 'N/A'
        age = extract_build_year(age)
        # å®¶è³ƒ
        rent_tag = content.find('span', class_='cassetteitem_price cassetteitem_price--rent')
        rent = rent_tag.text.strip() if rent_tag else 'N/A'
        # ç®¡ç†è²»
        fee_tag = content.find('span', class_='cassetteitem_price cassetteitem_price--administration')
        fee = fee_tag.text.strip() if fee_tag else 'N/A'
        # é–“å–ã‚Š
        madori_tag = content.find('span', class_='cassetteitem_madori')
        madori = madori_tag.text.strip() if madori_tag else 'N/A'
        if madori == 'ãƒ¯ãƒ³ãƒ«ãƒ¼ãƒ ':
            madori = '1R'
        else:
            madori = madori
        # é¢ç©
        menseki_tag = content.find('span', class_='cassetteitem_menseki')
        menseki = menseki_tag.text.strip() if menseki_tag else 'N/A'
        # éšæ•°
        kaisu_tag = content.find('tr', class_='js-cassette_link')
        kaisu = kaisu_tag.text.strip() if kaisu_tag else 'N/A'
        kaisu = extract_floor(kaisu)
        # è©³ç´°ãƒšãƒ¼ã‚¸URL
        link_tag = content.find('a', class_='js-cassette_link_href')
        link = 'https://suumo.jp' + link_tag.get('href') if link_tag else 'N/A'

        d_list.append({
            'ç‰©ä»¶å': title,
            #'é§…ãƒ»å¾’æ­©': station,
            'ç¯‰å¹´æ•°': age,
            #'éšæ•°': kaisu,
            'å®¶è³ƒ': rent,
            'ç®¡ç†è²»': fee,
            #'é–“å–ã‚Š': madori,
            'é¢ç©': menseki,
            'URL': link
        })

# LINE Messaging APIã®ãƒãƒ£ãƒãƒ«ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³
LINE_ACCESS_TOKEN = os.getenv("LINE_ACCESS_TOKEN")
if LINE_ACCESS_TOKEN is None:
    print("ç’°å¢ƒå¤‰æ•° LINE_ACCESS_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
else:
    print("ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ãŒæ­£å¸¸ã«å–å¾—ã§ãã¾ã—ãŸ")

def send_line_message(message):
    """LINEã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ã‚‹"""
    url = "https://api.line.me/v2/bot/message/broadcast"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {LINE_ACCESS_TOKEN}"
    }
    import json
    data = {
        "messages": [
            {"type": "text", "text": message}]
    }
    response = requests.post(url, json=data, headers=headers)
    return response.status_code

# å„è¡Œã‚’ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã«å¤‰æ›
df_new = pd.DataFrame(d_list)
today_str = datetime.today().strftime('%Y%m%d')
filename = f"{today_str}_suumo.csv"
df_new.to_csv(filename, index=False)
print(f"æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

# 1å›å‰ã®ãƒ‡ãƒ¼ã‚¿ã¨æ¯”è¼ƒ
prev_files = sorted([f for f in os.listdir() if f.endswith("_suumo.csv")])
if len(prev_files) > 1:
    prev_file = prev_files[-2]
    df_prev = pd.read_csv(prev_file)
    df_diff = df_new[~df_new['URL'].isin(df_prev['URL'])]

    if not df_diff.empty:
        diff_filename = f"{today_str}_suumo_new.csv"
        df_diff.to_csv(diff_filename, index=False)
        print(f"æ–°è¦ç‰©ä»¶ã®ã¿ã‚’ {diff_filename} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    else:
        print("æ–°è¦ç‰©ä»¶ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
else:
    print("æ¯”è¼ƒç”¨ã®éå»ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

# æ–°è¦ç‰©ä»¶ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã«ã®ã¿ã€ç‰©ä»¶ã‚’é€šçŸ¥
if os.path.exists(f"{today_str}_suumo_new.csv"):
    print("æ–°è¦ç‰©ä»¶æƒ…å ±ãŒã‚ã‚Šã¾ã™ã€‚")
    df2 = pd.read_csv(f"{today_str}_suumo_new.csv")
    # å„è¡Œã‚’ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã«å¤‰æ›
    text = df2.to_string(index=False)

    # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    with open("output.txt", "w", encoding="utf-8") as f:
        f.write(text)

    print(text)  # ç¢ºèªç”¨

    # 1. ã‚¿ãƒ–ï¼ˆ\tï¼‰ã€æ”¹è¡Œï¼ˆ\n, \rï¼‰ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®æ›
    text = re.sub(r'[\t\n\r]+', ' ', text)
    text = text.replace("\\r", "")
    text = text.replace("\\t", "")
    text = text.replace("\\n", "")
    text = text.replace("\n", "")
    text = text.replace("\t", "")
    text = text.replace("\r", "")

    # è¿½åŠ ã™ã‚‹æ–‡
    intro_text = "ğŸ¡æ–°ç€ç‰©ä»¶æƒ…å ±ã®ãŠçŸ¥ã‚‰ã›ğŸ¡\nã“ã‚“ã«ã¡ã¯ï¼\nä½ã¾ã„ã‚³ãƒ³ã‚·ã‚§ãƒ«ã‚¸ãƒ¥ã§ã™âœ¨\nä»Šé€±ã®ãŠã™ã™ã‚ç‰©ä»¶ã‚’ãŠå±Šã‘ã—ã¾ã™ï¼\n"
    # textã®æœ€åˆã«è¿½åŠ 
    text = intro_text + text
    last_text="æ°—ã«ãªã‚‹ç‰©ä»¶ãŒã‚ã‚Šã¾ã—ãŸã‚‰ã€ãŠæ—©ã‚ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ï¼ğŸ˜Š\næ¬¡å›ã®æ›´æ–°ã‚‚ãŠæ¥½ã—ã¿ã«âœ¨"
    text = text + last_text
    # 2. é€£ç¶šã™ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«çµ±ä¸€
    text = re.sub(r'\s+', ' ', text).strip()

    # LINEé€šçŸ¥é€ä¿¡
    if len(text) > 500:
        messages = [text[i:i+500] for i in range(0, len(text), 500)]
        for msg in messages:
            status = send_line_message(msg)
            print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {status}")
    else:
        status = send_line_message(text)
        print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {status}")
else:
    text="ä»Šå›ã¯æ–°ç€ç‰©ä»¶ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    # è¿½åŠ ã™ã‚‹æ–‡
    intro_text = "ğŸ¡æ–°ç€ç‰©ä»¶æƒ…å ±ã®ãŠçŸ¥ã‚‰ã›ğŸ¡\nã“ã‚“ã«ã¡ã¯ï¼\nä½ã¾ã„ã‚³ãƒ³ã‚·ã‚§ãƒ«ã‚¸ãƒ¥ã§ã™âœ¨\n"
    # textã®æœ€åˆã«è¿½åŠ 
    text = intro_text + text
    last_text="æ¬¡å›ã®æ›´æ–°ã‚‚ãŠæ¥½ã—ã¿ã«âœ¨"
    text = text + last_text
    # 2. é€£ç¶šã™ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«çµ±ä¸€
    text = re.sub(r'\s+', ' ', text).strip()
    # LINEé€šçŸ¥é€ä¿¡
    status = send_line_message(text)
    print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {status}")